# This source tails the logs that are created by the Docker json-file logging driver
<source>
  @type tail
  tag docker.*
  path /var/lib/docker/containers/*/*-json.log
  pos_file /tmp/docker.pos
  format json
  refresh_interval 10s
  time_key time
  time_format %FT%T
</source>

# This creates 2 fields for getting more context on Docker containers, cont_id and cont_meta_data -- which is simply a JSON load of the config.v2.json created by the JSON-logging driver
<filter docker.**>
  @type record_transformer
   enable_ruby
   <record>
    cont_id ${tag.split('.')[5]}
# --This is for adding the complete set of container metadata available and loads it into a field but does not JSON parse it--
    cont_meta_data ${id = tag.split('.')[5]; IO.read("/var/lib/docker/containers/#{id}/config.v2.json")}
#    type "docker-logs"
  </record>
</filter>

#This section actually parses the cont_meta_data field as JSON and attaches it to the record with the prefix dockermetadata.
<filter docker.**>
  @type parser
  format json
  key_name cont_meta_data
  reserve_data true
  hash_value_field dockermetadata
</filter>

# Code that pulls out a particular key -- in this case Docker Image and makes it the type
<filter docker.**>
  @type record_transformer
  enable_ruby
  <record>
    type ${record['dockermetadata']['Config']['Image'].split(':')[0]}
  </record>
</filter>

#This code processes multi-line messages such as stack-traces and corresponds to the stacktrace.examples provided here
<filter docker.**>
  @type concat
  key log
  stream_identity_key cont_id
  multiline_start_regexp /^\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}.*/
  continuous_line_regexp /^[\t\s]+at.*/
</filter>

#This copies the FluentD field log over to message for increased usability within the Kibana Discover page
<match docker.**>
  @type record_reformer
  renew_record false
  enable_ruby false
  tag droprecords
  <record>
    message ${log}
  </record>
</match>

#This drops unnecessary keys and unusued Docker metadata. It also changes the FluentD tag to the 'type' for routing to multiple sub-accounts
<match droprecords>
  @type record_reformer
  remove_keys dockermetadata,cont_meta_data,log
  renew_record false
  enable_ruby false
  tag ${type}
</match>

#This is a match specifically for ubuntu logs and routes them to the first Logz.io sub-account
<match ubuntu>
      type logzio_buffered
      endpoint_url https://listener.logz.io:8071?token=xxxxxxxxxsubaccount1
      output_include_time true
      output_include_tags true
      buffer_type    file
      buffer_path    /tmp/fluentd/logsz_buffer
      flush_interval 1s
      buffer_chunk_limit 1m   # Logz.io has bulk limit of 10M. We recommend set this to 1M, to avoid oversized bulks
</match>

#This is a catch-all match which routes all remaining events to a different sub-account
<match **>
      type logzio_buffered
      endpoint_url https://listener.logz.io:8071?token=xxxxxxxxxxsubaccount2
      output_include_time true
      output_include_tags true
      buffer_type    file
      buffer_path    /tmp/fluentd/logsz_buffer
      flush_interval 1s
      buffer_chunk_limit 1m   # Logz.io has bulk limit of 10M. We recommend set this to 1M, to avoid oversized bulks
</match>